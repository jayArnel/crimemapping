%*******************************************************************************
%*********************************** Second Chapter *****************************
%*******************************************************************************

\chapter{Review of Related Literature}  %Title of the Second Chapter

%********************************** %First Section  **************************************
\section{Crime} %Section - 2.1
    The formal definition of crime varies widely among criminologists but Edwin Sutherland’s short one may be sufficient. According to him, crime is “[a]n unlawful act is not defined as criminal by the fact that it is punished, but by the fact that it is punishable.” \cite{brown2010criminology} But what is punishable may be relative to what the crime is and where it happened. As Sutherland continued, an act can be considered crime in its essentiality once the State as which the jurisdiction of the act prevails deems such act as an injury and punishable by law.

    In the Philippines, as per the Philippine National Police, crimes can be classified into two: index crimes and non-index crimes \cite{senate2013criminal}. Index crimes are crimes that involves victims such as murder, homicide, physical injury and rape; or against property such as robbery, theft, and burglary. On the other hand, non-index crimes are violations of special laws such as illegal logging or local ordinances. In the Philippines, these classifications are used for statistical purposes and to create a standardized definition of crime classification. \cite{tumulak2015crime}

%********************************** %Second Section  **************************************
\section{Crime Prediction Techniques} %Section - 2.2
    In order to alleviate crime, criminologists are seeking techniques and methods in criminal statistics and analysis to help them prevent and approach crimes better. Machine learning techniques geared towards crime modelling and prediction have already been helping police officials and criminologist in dealing with criminal behaviors.

    A study entitled “Spatio-Temporal Crime Prediction Model Based on Analysis of Crime Clusters” \cite{polat2007spatio} aims to prevent and reduce crime and manage security resources efficiently by forecasting crime. The study area is the Çankaya district of Ankara, the capital city of Turkey. The data used in the study included the number, address, occurrence time, location and type of crime – murder, usurp, burglary, auto related crimes and pickpocketing. Other data such as landmarks and land-use were also included. In the study, clusters are generated and compared with respect to land-use, algorithm, covered area, and suitability to a spatio-temporal crime prediction model. The clustering algorithms to be compared were K-means, Nnh hierarchical, Spatio-temporal Analysis of Crime (STAC), fuzzy, ISODATA, and Geographical Analysis Machine (GAM). The study also aims to determine the most suitable distance metric in implementing clustering. The distance metrics in concern are Euclidean distance and Manhattan distance. After implementing all clustering algorithms, it was found out that K-means, fuzzy and ISODATA can cover all observations in the study area but are spatially inefficient and has difficulty to detect dense crime areas. It was also found that the Nearest neighbor hierarchical approach is not meaningful, is ineffective, and yields too many and too small clusters. The best algorithm was STAC, which showed dense crime areas and covers most area. It includes more homogenous areas than other methods and indicates denser crime areas than other methods. It is also computationally efficient. As for the distance metric, the Manhattan distance clusters has lower surface area than Euclidean distance clusters. The spatio-temporal crime prediction model had a mean root square error of 1.08 and 1.48 for Manhattan and Euclidean distance applications, respectively. The study recommends to investigate other clustering algorithms to apply the research to another study area.

    Another study called “Modeling and Mapping Crime in Eastern Nairobi, Kenya” \cite{mburu2014modeling} also aims in geospatial crime mapping but this time in Nairobi, Kenya. The study uses a dataset of 1,422 solved crimes constituting 346 series of 10 different crime types. It has three phases: criminal geographic profiling, space and time crime surveillance and future crime prediction. Criminal geographic profiling is a search prioritization strategy that analyzes the locations of already known crimes in order to predict the residence (or other anchor point) of a serial offender. Space and time crime surveillance is a mixture of time series analysis with kernel density estimation and the space-time permutation scan statistic to monitor patterns over a period of time. Lastly, future crime prediction uses regression analysis to model the relationship between the socio-economic factors in the study area and crime events. The different phases yield different ways to predict criminal activities. Criminal geographic profiling produces a probability surface of crime occurrence. The space and time crime surveillance identifies of crime clusters in space and time, and subsequently identifies of early crime-detection surveillance. Lastly, future crime prediction identifies patterns that will enable prediction of future crime levels.

    Social media can also be important instruments in predicting crime. A study called “Predicting Crime Using Twitter and Kernel Density Estimation” \cite{gerber2014predicting} aims to utilize social media, particularly Twitter, to predict local criminal activity. The study used criminal records from the Chicago Police Department. Tweets tagged with GPS coordinates falling within the city limits of Chicago, Illinois were collected. The criminal records and the tweets are within the same time period. The study uses a two-dimensional spatial probability density function (kernel density estimation) to a historical crime record. The tweets were first tokenized and tagged and were run through the MALLET toolkit to output probable topics. The KDE was then used to map criminal activities extracted as topics from gathered tweets. A surveillance plot measures the percentage of true T crimes during the prediction window (y-axis) that occur within the x\% most threatened area according to the model's prediction for T and it was shown that addition of Twitter-derived features improves prediction performance for 19 of 25 crime types.

\section{Deep Learning}
    Deep learning is a relatively new field of machine learning \cite{deng2014deep}. It is defined to be a set of techniques that employ multiple layers of non-linear information processing for feature extraction and transformation, and for pattern analysis and classification. These techniques have already been used for a wide range of signal and information processing work. Deep learning has gained popularity due to improved computational capabalities of chips (graphic processing units or GPUs), the emergence of big data and recent advancements in machine learning research. One of the most successful deep learning techniques is the Long Shot-term Memory (LSTM) network \cite{schmidhuber2015deep}, a kind of Recurrent Neural Network (RNN).

\section{Recurrent Neural Networks}

    Neural networks are one of the most commonly used machine learning algorithm. But despite their success, traditional neural networks are still faced with the incapacity to predict patterns in time and allow persistence of information. Ironically, neural networks were designed to emulate the connections of the neurons in our brain to model decisions but unlike neural networks, human thoughts have persistence. We derive our decisions by accumulating experience and using them as factors in calculating outcomes and making judgements. A factor may be heavier in one decision than another. Different situations demand different weighing of experience. Our decisions are not just affected by the situation itself but from our previous decisions. Unfortunately, traiditional neural networks does not allow persistence of information to have effect on its future predictions.

    Recurrent neural networks (RNN) solve this predicament. These networks have loops in them that carries persistent information. In traditional neural networks, hidden neurons have weighted connections only to the outputs. In RNNs, in addition to a feed to the output, a hidden neuron has a weighted connection to the hidden layers of the next time step, represented as a loop to itself. This connection represents the persistence of data throughout the timesteps/iterations of data in the network.

\section{Long Short-term Memory Networks}
